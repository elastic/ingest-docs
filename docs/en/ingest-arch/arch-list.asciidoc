[[use-case-arch]]
== Ingest architectures and use cases

IMPORTANT: The numbers shown in this table correspond to the numbered architectures in the source documents for easier tracking, and will be removed before we go live. 
This table probably won't appear in the finished documentation, certainly not in its current form.
But for now, it's helpful for grouping and quantifying work, and for showing the current set of architectures at a glance. 

Reference: https://www.elastic.co/guide/en/cloud/current/ec-cloud-ingest-data.html#ec-data-ingest-pipeline[Data ingestion pipeline]

[cols="50, 50"]
|===
| *Ingest architecture* | *Use cases and "how to's"*

| {agent} to {es} 
a| 
* 1) Elastic integration is available
** 1a) Software components with agent installed
** 1b) Software components with APIs for data collection
* 2) Software components with customizable Elastic integration available
** 2a) Data from syslog server: TCP/UDP (custom)
** 2b) Data from scraping an external API: Httpjson (custom)
** 2c) Data pushed from external API to HTTP endpoint: Httpinput (custom)
** 2d) Data read from disk: Custom logs [category]
* 3) Elastic integration is available and you want to modify schema
** 3a) Runtime fields use cases
** 3b) Ingest pipelines use cases
** 3c) Agent processors use cases
* 4) Agent needs enrichment (outside the stack or at collection)
* 5) Buffer at the edge (with Filebeat disk queue or Metricbeat memory queue). Endpoint is buffered on disk up to 500MB). 

| Application with ECS Logging -> {agent} to {es}
a|
* 6a) https://www.elastic.co/guide/en/ecs-logging/overview/current/intro.html[Application logging with ECS conformance]

| Filebeat modules -> {es}
a|
* 6b) ToDo: Update doc (which doc???) to point to integrations instead of modules. [KBM: If we're pointing to integrations instead of modules, I'm not sure why this one is here and not listed in the integration section.]

| Serverless Forwarder -> {es}
a|
* 7) Details TBD

| {ls} -> {es}
a|
* 8) Data from source that {agent} can't read (such as databases, AWS Kinesis): Check out {logstash-ref}/input-plugins.html. 

| (reference data: {ls} -> {es}) + {agent} -> {es}
a|
* 9) Scrape enrichment data from source and send it to the {es} cluster where data from {es} is ingested

| {agent} -> {ls} -> {es}
a|
* 10) {ls} for enrichment between {agent} and {es}
* 11) {ls} to collect enrichment data based on fields in {agent} data
* 12) {ls} persistent queue for buffering (to accomodate network issues and downstream unavailability)

| {agent} -> {ls} -> [Destination1, additional destinations]
a|
* 13) Agent collected data needs to be routed to different ES clusters or non-ES destinations depending on the content

| (Data path: {agent} -> {ls} -> {es}) + (Control path: {agent} -> {fleet-server} -> {es})
a|
* 14) Agents have network restrictions for connecting outside of agent network: https://www.elastic.co/guide/en/fleet/current/fleet-agent-proxy-support.html[Proxy server with Fleet]

| (Data path: {agent} -> Proxy -> {es}) + (Control path: {agent} -> Proxy + {fleet-server} -> {es})
a|
* 15) Agents have network restrictions for connecting outside of agent network: https://www.elastic.co/guide/en/fleet/current/fleet-agent-proxy-support.html[Proxy server with Fleet]  TODO: Differentiant with #14. 

| {agent} -> {ls} -> Kafka -> Kafka ES Sink -> {es}
a|
* 16a) Kafka as middleware message queue: Kafka ES sink connector to write from kafka to {es}

| {agent} -> {ls} -> Kafka -> {ls} -> {es}
a|
* 16b) Kafka as middleware message queue: {ls} to read from Kafka and route to {es}

| Kafka -> {es} Sink Connector/{ls} -> {es}
a|
* 17a) Kafka -> ES Sink Connector -> ES
* 17b) Kafka -> {ls} -> ES
* 18) {agent} -> {ls} -> Kafka -> ES Sink Connector -> ES

| Custom Beat to LS input 
a|
* 19) When no inputs are available, explore custom beat/logstash input plug-ins (Details TBD)

| {agent}/{stack}: air gapped environment
a|
* 20) Agent, Elastic stack both deployed in air gapped environment with no access to outside network



|===
