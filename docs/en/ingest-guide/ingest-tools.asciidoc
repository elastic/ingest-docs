[[ingest-tools]]
== Tools for ingesting data 

// Iterative messaging as our recommended strategy morphs. 
// This section is the summary. "Here's the story _now_." 
// Hint at upcoming changes, but do it cautiously and responsibly. 
// Modular and co-located to make additions/updates/deprecations easier as our story matures.

Elastic and others offer tools to help you get your data from the original data source into {es}.
Some tools are best suited for certain data sources, and others are multi-purpose.  

Elastic agent and Elastic integrations::
You can install a single Elastic Agent and collect a variety of data types from a single host computer.  
You can manage all of your agents and policies with the Fleet UI in {kib}. 
+
Use Elastic Agent with one of hundreds of Elastic integrations to simplify collecting, transforming, and visualizing data. 
Integrations include default ingestion rules, dashboards, and visualizations to start analyzing your data right away. 
Check out the {integrations-docs}/all_integrations[Integration quick reference] to search for available integrations that can reduce your time to value.  
+
Elastic Agent is the best approach for collecting timestamped data for most data sources and use cases. 
If you want to use the features of Elastic Agent but need additional processing, consider adding Agent processors or Logstash.
//ToDo: Add info on agent processors, Logstash inputs/filters/output, and Logstash integration filter. 
+ 
**Beats.** Beats are the original Elastic lightweight data shippers, and their capabilities live on in Elastic Agent.
When you use Elastic Agent, you're getting core Beats functionality and more added features. 
Beats requires that you install a separate Beat for each type of data you want to collect. 
A single Elastic Agent installed on a host can collect multiple types of data.  
+
Best practice: Use Elastic Agent whenever possible. 
If your data source is not yet supported by Elastic Agent, use Beats
Check out Beats and Agent https://www.elastic.co/guide/en/fleet/current/beats-agent-comparison.html#additional-capabilities-beats-and-agent[capabilities comparison] for more info. 

OpenTelemetry (OTel) collectors:: 
link:https://opentelemetry.io/docs[OpenTelemetry] is a vendor-neutral observability framework for collecting, processing, and exporting telemetry data.
Elastic is a member of the Cloud Native Computing Foundation (CNCF) and active contributor to the OpenTelemetry project. 
+
In addition to supporting upstream OTel development, Elastic provides link:https://github.com/elastic/opentelemetry[Elastic Distributions of OpenTelemetry], specifically designed to work with Elastic Observability. 

Logstash:: 
{ls} is a versatile open source data ETL (extract, transform, load) engine that can expand your ingest capabilities.
{ls} can _collect data_ from a wide variety of data sources with {ls} link:{logstash-ref}/input-plugins.html[input
plugins], _enrich and transform_ the data with {ls} link:{logstash-ref}/filter-plugins.html[filter plugins], and _output_ the
data to {es} and other destinations with the {ls} link:{logstash-ref}/output-plugins.html[output plugins].
Many users never need to use {ls}, but it's there if you need it. 
+
One of the most common {ls} use cases is link:{logstash-ref}/ea-integrations.html[extending Elastic integrations].
You can take advantage of the extensive, built-in capabilities of Elastic Integrations, and
then use {ls} for additional data processing before sending the data on to {es}. 
+
{ls} can help with advanced use cases such as the need for additional
link:https://www.elastic.co/guide/en/ingest/current/ls-enrich.html[data
enrichment],  
link:https://www.elastic.co/guide/en/ingest/current/lspq.html[persistence or
buffering],
link:https://www.elastic.co/guide/en/ingest/current/ls-networkbridge.html[proxying]
to bridge network connections, or the ability to route data to
link:https://www.elastic.co/guide/en/ingest/current/ls-multi.html[multiple
destinations].

Elasticsearch ingest pipelines::
{es} link:{ref}/ingest.html[ingest pipelines] let you complete common transformations on your data before the data is indexed. 
//ToDo: Explain when to use

Language clients:: 
link:https://www.elastic.co/guide/en/elasticsearch/client/index.html[Elastic
language clients] help you send **application data**, such as from NodeJS or Python,
directly to {es} for search and analysis. 
//ToDo: Figure out trademark considerations.

APIs::
Use the {es} document APIs to index **documents** directly into {es}.

File uploader::
Use the {kib} file uploader to index **single files** into {es}.
This tool can be helpful for testing with small numbers of files. 

Web crawler::
Use the Elastic web crawler to index **web page content**.

Connectors::
Use connectors to index **data from third-party sources**, such as Amazon S3, GMail, Outlook, and Salesforce.
//ToDo: Figure out trademark considerations. 

Elastic serverless forwarder::
The Elastic Serverless Forwarder is an Amazon Web Services (AWS) Lambda function that ships logs from your AWS environment to {es}.

[discrete]
[[ingest-addl-proc]]
== Tools for additional processing

* link:[Agent processors] for sanitizing or enriching raw data at the source
* {es} link:{ref}/[ingest pipelines] for enriching incoming data 
// ToDo: Decide if ingest pipelines should be included for additional processing. 
// Decision tree lists ECS-ifying or normalizing field data, but that should be handled by Agent.
// Does that leave enriching data as the only enrichment use case?
// Does the Logstash elastic_integration filter remove that one, also?
* {es} link:{ref}/runtime.html[runtime fields]
* {ls} link:[elastic_integration filter]
* {ls} link:[filter plugins]
