[[monitor-elastic-agent]]
= Monitor {agent}s

{fleet} provides built-in capabilities for monitoring your fleet of {agent}s.
In {fleet}, you can:

* <<view-agent-status>>
* <<view-agent-details>>
* <<view-agent-activity>>
* <<view-agent-logs>>
* <<collect-agent-diagnostics>>
* <<view-agent-metrics>>
* <<change-agent-monitoring>>
* <<external-elasticsearch-monitoring>>
* <<fleet-alerting>>

Agent monitoring is turned on by default in the agent policy unless you
turn it off. Want to turn off agent monitoring to stop collecting logs and
metrics? See <<change-agent-monitoring>>.

[discrete]
[[view-agent-status]]
= View agent status overview

To view the overall status of your {fleet}-managed agents, in {kib}, go to
**Management -> {fleet} -> Agents**.

[role="screenshot"]
image::images/agent-status.png[Agents tab showing status of each {agent}]

{agent}s can have the following statuses:

|===

| *Healthy* | {agent}s are enrolled and checked in. There are no agent policy updates or automatic agent binary updates in progress, but the agent binary may still be out of date. {agent}s continuously check in to the {fleet-server} for required updates. 

| *Unhealthy* | {agent}s have not checked in to {fleet-server}. At this point, you may need to address the situation. 

| *Updating* | {agent}s are updating the agent policy, updating the binary, or enrolling or unenrolling from {fleet}.

| *Offline* | {agent}s have stayed in an unhealthy status for a period of time. Offline agent's API keys remain valid. You can still see these {agent}s in the {fleet} UI and investigate them for further diagnosis if required.

| *Inactive* | {agent}s have been offline for longer than the time set in your <<set-inactivity-timeout,inactivity timeout>>. These {agent}s are valid, but have been removed from the main {fleet} UI. 

| *Unenrolled* | {agent}s have been manually unenrolled and their API keys have been removed from the system. You can <<unenroll-elastic-agent,unenroll>> an offline {agent} using {agent} actions if you determine it's offline and no longer valid. 

These agents need to re-enroll in {fleet} to be operational again.

|===

The following diagram shows the flow of {agent} statuses:

image::images/agent-status-diagram.png[Diagram showing the flow of Fleet Agent statuses]

To filter the list of agents by status, click the **Status** dropdown and select
one or more statuses.

[role="screenshot"]
image::images/agent-status-filter.png[Agent Status dropdown with multiple statuses selected]

For advanced filtering, use the search bar to create structured queries
using {kibana-ref}/kuery-query.html[{kib} Query Language]. For example, enter
`local_metadata.os.family : "darwin"` to see only agents running on macOS.

[discrete]
[[view-agent-details]]
= View details for an agent

In {fleet}, you can access the detailed status of an individual agent and the integrations that are associated with it through the agent policy.

. In {kib}, go to **Management -> {fleet} -> Agents**.

. In the **Host** column, click the agent's name.

On the **Agent details** tab, the **Overview** pane shows details about the agent and its performance, including its memory and CPU usage, last activity time, and last checkin message. To access metrics visualizations, you can also <<view-agent-metrics>>.

image::images/agent-detail-overview.png[Agent details overview pane with various metrics]

The **Integrations** pane shows the status of the integrations that have been added to the agent policy. Expand any integration to view its health status. Any errors or warnings are displayed as alerts.

image::images/agent-detail-integrations-health.png[Agent details integrations pane with health status]

To gather more detail about a particular error or warning, from the **Actions** menu select **View agent JSON**. The JSON contains all of the raw agent data tracked by Fleet.

NOTE: Currently, the **Integrations** pane shows the health status only for agent inputs. Health status is not yet available for agent outputs.

[discrete]
[[view-agent-activity]]
= View agent activity

You can view a chronological list of all operations performed by your {agent}s.

On the **Agents** tab, click **Agent activity**. All agent operations are shown, beginning from the most recent, including any in progress operations.

[role="screenshot"]
image::images/agent-activity.png[Agent activity panel, showing the operations for an {agent}]

[discrete]
[[view-agent-logs]]
= View agent logs

When {fleet} reports an agent status like `Offline` or `Unhealthy`, you might
want to view the agent logs to diagnose potential causes. If agent monitoring
is configured to collect logs (the default), you can view agent logs in {fleet}.

. In {kib}, go to **Management -> {fleet} -> Agents**.

. In the **Host** column, click the agent's name.

. On the **Agent details** tab, verify that **Monitor logs** is enabled. If
it's not, refer to <<change-agent-monitoring>>.

. Click the **Logs** tab.
+
[role="screenshot"]
image::images/view-agent-logs.png[View agent logs under agent details]

On the **Logs** tab you can filter, search, and explore the agent logs:

* Use the search bar to create structured queries using
{kibana-ref}/kuery-query.html[{kib} Query Language].
* Choose one or more datasets to show logs for specific programs, such as
{filebeat} or {fleet-server}.
+
[role="screenshot"]
image::images/kibana-fleet-datasets.png[{fleet} showing datasets for logging]
* Change the log level to filter the view by log levels. Want to see debugging
logs? Refer to <<change-logging-level>>.
* Change the time range to view historical logs.
* Click **Open in Logs** to tail agent log files in real time. For more
information about logging, refer to
{observability-guide}/tail-logs.html[Tail log files].

[discrete]
[[change-logging-level]]
= Change the logging level

The logging level for monitored agents is set to `info` by default. You can
change the agent logging level, for example, to turn on debug logging remotely:

. After navigating to the **Logs** tab as described in <<view-agent-logs>>,
scroll down to find the **Agent logging level** setting.
+
[role="screenshot"]
image::images/agent-set-logging-level.png[{Logs} tab showing the agent logging level setting]

. Select an *Agent logging level*:
+
|===
a|`error` | Logs errors and critical errors.
a|`warning`| Logs warnings, errors, and critical errors.
a|`info`| Logs informational messages, including the number of events that are published.
Also logs any warnings, errors, or critical errors.
a|`debug`| Logs debug messages, including a detailed printout of all events flushed. Also
logs informational messages, warnings, errors, and critical errors.
|===

. Click **Apply changes** to apply the updated logging level to the agent.

[discrete]
[[collect-agent-diagnostics]]
= Collect {agent} diagnostics

{fleet} provides the ability to remotely generate and gather an {agent}'s diagnostics bundle.
An agent can gather and upload diagnostics if it is online in a `Healthy` or `Unhealthy` state.
To download the diagnostics bundle for local viewing:

. In {kib}, go to **Management -> {fleet} -> Agents**.

. In the **Host** column, click the agent's name.

. Click the **Diagnostics** tab.

. Click the **Request diagnostics .zip** button.
+
[role="screenshot"]
image::images/collect-agent-diagnostics.png[Collect agent diagnostics under agent details]

Any in-progress or previously collected bundles for the {agent} will be listed on this page.

Note that the bundles are stored in {es} and will be removed after 30 days.

[discrete]
[[view-agent-metrics]]
= View the {agent} metrics dashboard

When agent monitoring is configured to collect metrics (the default), you can
use the **[Elastic Agent] Agent metrics** dashboard in {kib} to view details
about {agent} resource usage, event throughput, and errors. This information can
help you identify problems and make decisions about scaling your deployment.

To view agent metrics:

. In {kib}, go to **Management -> {fleet} -> Agents**.

. In the **Host** column, click the agent's name.

. On the **Agent details** tab, verify that **Monitor metrics** is enabled. If
it's not, refer to <<change-agent-monitoring>>.

. Click **View more agent metrics** to navigate to the
**[Elastic Agent] Agent metrics** dashboard.
+
[role="screenshot"]
image::images/selected-agent-metrics-dashboard.png[Screen capture showing {agent} metrics]

The dashboard uses standard {kib} visualizations that you can extend to meet
your needs.

[discrete]
[[change-agent-monitoring]]
= Change {agent} monitoring settings

Agent monitoring is turned on by default in the agent policy. To change agent
monitoring settings for all agents enrolled in a specific agent policy:

. In {kib}, go to **Management -> {fleet} -> Agent policies**.

. Click the agent policy to edit it, then click **Settings**.

. Under **Agent monitoring**, deselect (or select) one or both of these
settings: **Collect agent logs** and **Collect agent metrics**.

. Save your changes.

To turn off agent monitoring when creating a new agent policy:

. In the **Create agent policy** flyout, expand **Advanced options**.

. Under **Agent monitoring**, deselect **Collect agent logs** and
**Collect agent metrics**.

. When you're done configuring the agent policy, click **Create agent policy**.

[discrete]
[[external-elasticsearch-monitoring]]
= Send {agent} monitoring data to a remote {es} cluster

You may want to store all of the health and status data about your {agents} in a remote {es} cluster, so that it's separate and independent from the deployment where you use {fleet} to manage the agents.

To configure a remote {es} cluster for your {agent} monitoring data:

. In {kib}, go to **Management -> {fleet} -> Settings**.

. In the **Outputs** section, select **Add output**.

. In the **Add new output** flyout, provide a name for the output and select **Remote Elasticsearch** as the output type.

. In the **Hosts** field, add the URL that agents should use to access the remote {es} cluster.

.. To find the remote host address, in the remote cluster open {kib} and go to **Management -> {fleet} -> Settings**.

.. Copy the **Hosts** value for the default output.

.. Back in your main cluster, paste the value you copied into the output **Hosts** field.

. Create a service token to access the remote cluster.

.. Below the **Service Token** field, copy the API request.

.. In the remote cluster, open the {kib} menu and go to **Management -> Dev Tools**.

.. Run the API request.

.. Copy the value for the generated token.

.. Back in your main cluster, paste the value you copied into the output **Service Token** field.

. Choose whether or not the remote output should be the default for agent monitoring. When set, {agent}s use this output to send data if no other output is set in the <<agent-policy,agent policy>>.

. Add any <<es-output-settings-yaml-config,advanced YAML configuration settings>> that you'd like for the output.

. Click **Save and apply settings**.

After the output is created, you can update an {agent} policy to use the new remote {es} cluster:

. In {kib}, go to **Management -> {fleet} -> Agent policies**.

. Click the agent policy to edit it, then click **Settings**.

. Set the **Output for agent monitoring** option to use the output that you configured in the previous steps.

. Click **Save changes**.

The remote {es} cluster is now configured.

[discrete]
[[fleet-alerting]]
= Enable alerts and ML jobs based on {fleet} and {agent} status

You can access the health status of {fleet}-managed {agents} and other {fleet} settings through internal {fleet} indices. This enables you to leverage various applications within the {stack} that can be triggered by the provided information. For instance, you can now create alerts and machine learning (ML) jobs based on these specific fields. Refer to the {kibana-ref}/alerting-getting-started.html[Alerting documentation] to learn how to define rules that can trigger actions when certain conditions are met.

This functionality allows you to effectively track an agent's status, and identify scenarios where it has gone offline, is experiencing health issues, or is facing challenges related to input or output.

The following datastreams and fields are available.

Datastream::
`metrics-fleet_server.agent_status-default`
+
This data stream publishes the number of {agents} in various states.
+
**Fields**
+
 * `@timestamp`
 * `fleet.agents.total` - A count of all agents
 * `fleet.agents.enrolled` - A count of all agents currently enrolled
 * `fleet.agents.unenrolled` - A count of agents currently unenrolled
 * `fleet.agents.healthy` - A count of agents currently healthy
 * `fleet.agents.offline` - A count of agents currently offline
 * `fleet.agents.updating` - A count of agents currently in the process of updating
 * `fleet.agents.unhealthy` - A count of agents currently unhealthy
 * `fleet.agents.inactive` - A count of agents currently inactive

Datastream::
`metrics-fleet_server.agent_versions-default`
+
This index publishes a separate document for each version number and a count of enrolled agents only.
+
**Fields**
+
 * `@timestamp`
 * `fleet.agent.version` - A keyword field containing the version number
 * `fleet.agent.count` - A count of agents on the specified version
+
[NOTE] 
==== 
The following fields which indicate reasons why an agent is unhealthy are being considered for a later release:

* `fleet.agents.unhealthy_reason.input` - A count of agents unhealthy due to an input issue
* `fleet.agents.unhealthy_reason.output` - A count of agents unhealthy due to an output issue
* `fleet.agents.unhealthy_reason.other` - A count of agents unhealthy due to another issue
* `fleet.agents.upgrading_step.scheduled` - A count of agents in a scheduled upgrade state
====
